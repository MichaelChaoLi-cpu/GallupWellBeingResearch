


%pwd


%cd ..








import os 
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import xgboost as xgb











RawDataset_Filename = os.path.join("Data", "GallupWB_WashedRawData17Wave_v1.parquet")


RawDataset = pd.read_parquet(RawDataset_Filename)


RawDataset.shape


nan_counts = RawDataset.isna().sum()
for line in zip(RawDataset.columns, nan_counts):
    print(line)


nan_counts = RawDataset.isna().sum()
name_list = []
for line in zip(RawDataset.columns, nan_counts):
    if line[1] < 500_000:
        name_list.append(line[0])
        print(line)


name_list = []
for line in zip(RawDataset.columns, nan_counts):
    if line[1] < 200_000:
        name_list.append(line[0])
        print(line)


name_list = name_list + ['INCOME_2']


FineWashedDf = RawDataset[name_list]


FineWashedDf.shape


FineWashedDf.dropna(inplace=True)


FineWashedDf.shape








FineWashedDf = FineWashedDf.sample(frac=1, random_state=42).reset_index(drop=True)


FineWashedDf['COUNTRY_ISO3'] = FineWashedDf['COUNTRY_ISO3'].astype('category')


FineWashedDf.head()


y = FineWashedDf['WP16']


X = FineWashedDf.drop(columns=['WP16', 'WP18'])


X.shape


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)








RawDataset = None


FineWashedDf = None


X = None


y = None








model = xgb.XGBRegressor(objective='reg:squarederror', device = 'cuda', tree_method='hist', n_estimators=2000, learning_rate=0.05, max_depth=20, random_state=42, enable_categorical=True  )
model.fit(X_train, y_train)


y_pred = model.predict(X_test)


r2_score(y_test, y_pred) * 100


y_pred = model.predict(X_train)


r2_score(y_train, y_pred) * 100











RawDataset_Filename = os.path.join("Data", "GallupWB_WashedRawData17Wave_v1.parquet")


RawDataset = pd.read_parquet(RawDataset_Filename)


RawDataset.shape


nan_counts = RawDataset.isna().sum()
for line in zip(RawDataset.columns, nan_counts):
    print(line)








RawDataset['wave'].value_counts().sort_index()


RawDataset['wave'] = RawDataset['wave'].astype(int)


RawDataset['wave'].value_counts().sort_index()








group_means = RawDataset.groupby(['wave', 'COUNTRY_ISO3'])['INCOME_2'].transform('mean')


group_means


















